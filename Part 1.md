# Part 1

## Question 1.1

> ​	AlphaGO 不是神，而是魔鬼。魔鬼和神看似对立，其实有一个共同点——比人类强大万倍以上，所以人们往往分不清魔鬼和神。但是终究魔鬼是比不过神的，就像 AlphaGO 的围棋水平世界无人能敌，但是它只是一个使用了 heuristic 方法的深度强化学习程序，而不是一个穷尽了所有下法的谕示机（Oracle）。前者是魔鬼，后者是神。————匿名

​	人类与机器在棋类上的战争早已打响：当深蓝掀翻卡斯帕罗夫时，当李世石面对“第37手”时，手止不住的颤抖时；当柯洁赛后红着眼眶，说出“它太完美，我看不到希望时”……人类在绝对算力面前的渺小与悲壮，一览无余。人们一时间被AI的强大冲垮，对人类的智慧失去了信心，更担忧着世界的未来会不会被AI算尽……

​	但其实，答案早于那一行行代码中无声显露：AI并非全知全能的神，也无法做到了解一切解决一切，他的全能其实正是他专能的表现。他所做的一切一言以蔽之便是——计算。以当下为条件，以胜利为终点，一遍又一遍地计算着概率的海洋，赶着每一步下最汹涌的浪潮，然后将自己推向必胜的彼岸。他不是穷尽一切的上帝，只是一个勤勤恳恳的数学家。

​	这么看来，AI 的强大只是人类智慧的镜像，它所展现出的一切，归根结底是人类赋予它的能力。

​	AI 并不知道它模拟和计算出来的是什么，它没有对外界的感知，只是忠实地遵循人类给它的法则，执行人类给它的代码。它无法理解围棋的深邃意境，也无法体会人类在下棋时的喜怒哀乐。它只是在庞大的数据海洋中寻找模式和规律，然后按照既定的算法给出最有可能获胜的策略。就像一个被精心设计的机器，它可以在特定的轨道上高速运转，却无法跳出这个轨道去探索未知的世界。它没有情感的波动，也没有对胜利的渴望，只是机械地完成着人类赋予它的任务。

​	而人类则不同。人类在下棋时，不仅仅是在追求胜利，更是在享受棋局带来的乐趣和挑战。人类会因为一步妙手而欣喜若狂，也会因为一次失误而懊恼不已。这种情感的起伏，正是人类智慧的生动体现。更重要的是，人类拥有创造力和想象力，能够在看似无解的局面中找到“神之一手”。而 AI 只能基于已有的棋谱和算法进行推演，而更难具备创新性。

​	所以，尽管 AI 在棋类等特定领域展现出了惊人的能力，但它永远无法取代人类智慧的丰富性和独特性。它是人类智慧的延伸，而不是智慧本身。我们应该以更加自信和从容的心态去看待 AI 的发展，同时不断挖掘和发挥人类自身的优势，让人类智慧在与 AI 的协同中绽放出更加耀眼的光芒。



## Question 2

> ​	经过计算机学家和数学家的联合探索，人类已经发现了实现 Hex 智能的很多方法，其中出现最多的两种方法是 **alpha-beta** 剪枝结合评估函数和蒙特卡洛树搜索结合 **UCT** 算法。接下来我将比较这两种方法的异同：

### 基本介绍

- **alpha-beta 剪枝结合评估函数**

  **alpha-beta 剪枝**是一种基于极小化极大算法的优化方法，采用深度优先的方式搜索游戏树。通过 alpha-beta 剪枝，能有效减少不必要的分支搜索，提升效率。其搜索过程确定，每一步都严格遵循算法规则，剪枝决策明确。

  其核心想法是：若已知某节点的部分子节点的分数，虽然不能算出该节点的分数，但可以算出该节点的分数的取值范围。同时，利用该节点的分数的取值范围，在搜素其子节点时，如果已经确定没有更好的走法，就不必再搜索剩余的子节点了。通过这样的剪枝来减少搜索游戏树的时间，增加效率

- **蒙特卡洛树搜索结合 UCT 算法**：

  **蒙特卡洛树搜索（MCTS）** 是一种启发式搜索算法，它通过模拟大量随机局面来评估节点，从而避免了完全枚举所有可能状态的问题。MCTS 适用于状态空间巨大且计算复杂的游戏，比如围棋。MCTS 主要包括四个步骤：

  1. 选择（Selection）：从根节点开始，按照某种策略选择子节点，直到遇到未完全展开的节点。
  2. 扩展（Expansion）：扩展该节点，生成新的子节点。
  3. 模拟（Simulation）：从扩展的节点开始，随机进行一次游戏模拟，直到游戏结束。
  4. 反向传播（Backpropagation）：根据模拟结果更新树中节点的信息，通常更新访问次数和胜利次数。

### 主要区别

1. **确定性与随机性**

   alpha-beta 剪枝结合评估函数确定性算法，相同棋盘状态和参数下，多次运行结果一致，具有可重复性和验证性

   蒙特卡洛树搜索结合 UCT 算法引入随机变量，核心是通过概率来进行模拟，能更好地探索新策略。

2. **计算资源需求**

   alpha-beta 剪枝结合评估函数：计算资源需求相对较低，只需要根据算法稳步推进即可，适合计算资源有限场景。但一味地跟随算法可能陷入局部最优的情况，这种时候就要引入评估函数，以全局视角看待。

   蒙特卡洛树搜索结合 UCT 算法：计算资源需求较高，尤其在复杂游戏或时间限制严格时，需大量模拟才能获准确评估，响应实间长且慢。但其模拟质量更好，满足更多游戏的需求。

### 共同特征

- **在博弈中追求最优解**：两种算法的最终目标都是在游戏树中搜索出最优的行动方案，以最大化自身获胜的概率。无论是通过 alpha-beta 剪枝结合评估函数的确定性搜索，还是蒙特卡洛树搜索结合 UCT 算法的随机性探索，其本质都是在给定的游戏规则和棋盘状态下，为智能体找到最佳的下一步行动。他们都可以看作是Minimax和Negamax算法的衍生和优化。

- **尽可能提高搜索效率**：两种方法都在传统的游戏树搜索算法基础上进行了优化，以提高搜索效率。alpha-beta 剪枝通过剪去不可能影响最终决策的分支，减少了搜索的深度和广度；蒙特卡洛树搜索结合 UCT 算法则通过平衡探索与利用，对可能的情况进行海量的模拟，从而找到最胜策略。同时，二者也可以同时引入决策库进行进一步的优化响应速度，减少模拟情形。

### **适用场景**

alpha-beta 剪枝结合评估函数适用于评估函数易构建且计算资源有限的游戏，如国际象棋。其确定性行为和较低资源需求使其在资源受限设备上表现良好，因而国际象棋最先被AI攻克。蒙特卡洛树搜索结合 UCT 算法适用于评估函数难以构建或需处理不确定性的游戏，如围棋。其随机性和适应性使其在复杂、不确定性环境下更具优势，更容易产生创新性解法，也就是神之一手。

​	